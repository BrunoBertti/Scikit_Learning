{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.9.2. Multinomial Naive Bayes \n",
    "\n",
    "    ## MultinomialNB implementa o algoritmo Bayes ingênuo para dados distribuídos multinomialmente e é uma das duas variantes Bayes ingênuas clássicas usadas na classificação de texto (onde os dados são normalmente representados como contagens de vetores de palavras, embora os vetores tf-idf também funcionem bem na prática ) A distribuição é parametrizada por vetores \\ theta_y = (\\ theta_ {y1}, \\ ldots, \\ theta_ {yn}) para cada classe y, onde n é o número de características (na classificação do texto, o tamanho do vocabulário) e \\ theta_ {yi} é a probabilidade P (x_i \\ mid y) do traço i aparecer em uma amostra pertencente à classe y. \n",
    "\n",
    "    ## Os parâmetros \\ theta_y são estimados por uma versão suavizada de máxima verossimilhança, ou seja, contagem de frequência relativa:\n",
    "\n",
    "        ### \\ hat {\\ theta} _ {yi} = \\ frac {N_ {yi} + \\ alpha} {N_y + \\ alpha n}\n",
    "\n",
    "    ## onde N_ {yi} = \\ sum_ {x \\ in T} x_i é o número de vezes que o recurso i aparece em uma amostra da classe y no conjunto de treinamento T, e N_ {y} = \\ sum_ {i = 1} ^ { n} N_ {yi} é a contagem total de todos os recursos da classe y.\n",
    "\n",
    "\n",
    "    ## O smoothing priors \\ alpha \\ ge 0 leva em conta os recursos não presentes nas amostras de aprendizagem e evita zero probabilidades em cálculos posteriores. A configuração \\ alpha = 1 é chamada de suavização de Laplace, enquanto \\ alpha <1 é chamada de suavização de Lidstone. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
