{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.9. Naive Bayes\n",
    "\n",
    "    ## Os métodos Naive Bayes são um conjunto de algoritmos de aprendizagem supervisionada com base na aplicação do teorema de Bayes com a suposição \"ingênua\" de independência condicional entre cada par de recursos dado o valor da variável de classe. O teorema de Bayes afirma a seguinte relação, dada a variável de classe y e o vetor de característica dependente x através de x_n,:\n",
    "\n",
    "        ### P (y \\ mid x_1, \\ dots, x_n) = \\ frac {P (y) P (x_1, \\ dots, x_n \\ mid y)}\n",
    "        ###                         {P (x_1, \\ pontos, x_n)}\n",
    "\n",
    "    ## Usando a premissa ingênua de independência condicional de que\n",
    "\n",
    "        ### P (x_i | y, x_1, \\ pontos, x_ {i-1}, x_ {i + 1}, \\ pontos, x_n) = P (x_i | y),\n",
    "    \n",
    "    ##  para todos os i, esta relação é simplificada para\n",
    "\n",
    "        ### P (y \\ mid x_1, \\ dots, x_n) = \\ frac {P (y) \\ prod_ {i = 1} ^ {n} P (x_i \\ mid y)}\n",
    "        ###                         {P (x_1, \\ pontos, x_n)}\n",
    "        \n",
    "    ## Como P (x_1, \\ dots, x_n) é constante dada a entrada, podemos usar a seguinte regra de classificação:\n",
    "\n",
    "        ### \\ begin {align} \\ begin {alinhados} P (y \\ mid x_1, \\ dots, x_n) \\ propto P (y) \\ prod_ {i = 1} ^ {n} P (x_i \\ mid y) \\\\\\ Downarrow \\\\\\ hat {y} = \\ arg \\ max_y P (y) \\ prod_ {i = 1} ^ {n} P (x_i \\ mid y), \\ end {alinhado} \\ end {alinhar} \n",
    "\n",
    "    ## e podemos usar a estimativa de Máximo A Posteriori (MAP) para estimar P (y) e P (x_i \\ mid y); o primeiro é então a frequência relativa da classe y no conjunto de treinamento.\n",
    "\n",
    "    ## Os diferentes classificadores Bayes ingênuos diferem principalmente pelas suposições que fazem em relação à distribuição de P (x_i \\ mid y).\n",
    "\n",
    "\n",
    "    ## Apesar de suas suposições aparentemente simplificadas demais, os classificadores Bayes ingênuos funcionaram muito bem em muitas situações do mundo real, como a famosa classificação de documentos e filtragem de spam. Eles requerem uma pequena quantidade de dados de treinamento para estimar os parâmetros necessários. (Para razões teóricas pelas quais o ingênuo Bayes funciona bem, e sobre os tipos de dados que ele faz, veja as referências abaixo.)\n",
    "\n",
    "    ## Alunos e classificadores Naive Bayes podem ser extremamente rápidos em comparação com métodos mais sofisticados. O desacoplamento das distribuições de características condicionais de classe significa que cada distribuição pode ser estimada independentemente como uma distribuição unidimensional. Isso, por sua vez, ajuda a aliviar os problemas decorrentes da maldição da dimensionalidade.\n",
    "\n",
    "    ## Por outro lado, embora o ingênuo Bayes seja conhecido como um classificador decente, é conhecido por ser um estimador ruim, de modo que as saídas de probabilidade de predict_proba não devem ser levadas muito a sério. \n",
    "\n",
    "    ## Referências:\n",
    "\n",
    "    ## H. Zhang (2004). The optimality of Naive Bayes. Proc. FLAIRS. (https://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
